{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8a91696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bee18281",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cpu = torch.Generator()\n",
    "gen_cpu.manual_seed(215)      # for repeatability\n",
    "\n",
    "# generate  data\n",
    "N, D = 200, 2\n",
    "X = -5.0 + 10.0*torch.rand((N, D), generator=gen_cpu)\n",
    "y = torch.matmul(X, torch.tensor([5.0, -3.0])) + 0.3 * torch.randn((N,), generator=gen_cpu)\n",
    "\n",
    "\n",
    "max_iter = 100               # number of iterations\n",
    "init_lr = 1e-2              # learning rate of SGD, starting value \n",
    "meta_lr = 1e-4              # learning rate of the learning rate\n",
    "batch_size = 20\n",
    "loss_vals= []               # record the loss values, for plotting at the end\n",
    "lr_vals = []                # record the learning rate, for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7ee53c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 817.4296875\n",
      "Epoch 1, Loss: 318.187255859375\n",
      "Epoch 2, Loss: 285.124755859375\n",
      "Epoch 3, Loss: 178.11183166503906\n",
      "Epoch 4, Loss: 210.3045654296875\n",
      "Epoch 5, Loss: 237.78753662109375\n",
      "Epoch 6, Loss: 273.0526123046875\n",
      "Epoch 7, Loss: 226.8116455078125\n",
      "Epoch 8, Loss: 239.52171325683594\n",
      "Epoch 9, Loss: 238.94384765625\n",
      "Epoch 10, Loss: 181.41363525390625\n",
      "Epoch 11, Loss: 150.3371124267578\n",
      "Epoch 12, Loss: 261.02301025390625\n",
      "Epoch 13, Loss: 232.62911987304688\n",
      "Epoch 14, Loss: 155.97647094726562\n",
      "Epoch 15, Loss: 242.15953063964844\n",
      "Epoch 16, Loss: 296.1050109863281\n",
      "Epoch 17, Loss: 145.9178466796875\n",
      "Epoch 18, Loss: 198.56039428710938\n",
      "Epoch 19, Loss: 188.35116577148438\n",
      "Epoch 20, Loss: 198.0848388671875\n",
      "Epoch 21, Loss: 206.2211151123047\n",
      "Epoch 22, Loss: 325.4663391113281\n",
      "Epoch 23, Loss: 224.64678955078125\n",
      "Epoch 24, Loss: 256.341552734375\n",
      "Epoch 25, Loss: 170.00755310058594\n",
      "Epoch 26, Loss: 229.2094268798828\n",
      "Epoch 27, Loss: 273.3164978027344\n",
      "Epoch 28, Loss: 238.90106201171875\n",
      "Epoch 29, Loss: 179.77456665039062\n",
      "Epoch 30, Loss: 240.81829833984375\n",
      "Epoch 31, Loss: 184.72976684570312\n",
      "Epoch 32, Loss: 175.09556579589844\n",
      "Epoch 33, Loss: 268.70782470703125\n",
      "Epoch 34, Loss: 227.8697052001953\n",
      "Epoch 35, Loss: 186.15118408203125\n",
      "Epoch 36, Loss: 159.85897827148438\n",
      "Epoch 37, Loss: 193.70361328125\n",
      "Epoch 38, Loss: 214.4407196044922\n",
      "Epoch 39, Loss: 215.9354248046875\n",
      "Epoch 40, Loss: 277.841064453125\n",
      "Epoch 41, Loss: 226.4533233642578\n",
      "Epoch 42, Loss: 322.20648193359375\n",
      "Epoch 43, Loss: 247.46697998046875\n",
      "Epoch 44, Loss: 231.3755340576172\n",
      "Epoch 45, Loss: 222.4276123046875\n",
      "Epoch 46, Loss: 181.60140991210938\n",
      "Epoch 47, Loss: 145.8297576904297\n",
      "Epoch 48, Loss: 220.40667724609375\n",
      "Epoch 49, Loss: 157.82032775878906\n",
      "Epoch 50, Loss: 156.07574462890625\n",
      "Epoch 51, Loss: 222.69448852539062\n",
      "Epoch 52, Loss: 306.2337951660156\n",
      "Epoch 53, Loss: 255.85401916503906\n",
      "Epoch 54, Loss: 186.82664489746094\n",
      "Epoch 55, Loss: 350.24652099609375\n",
      "Epoch 56, Loss: 269.9612121582031\n",
      "Epoch 57, Loss: 188.6099090576172\n",
      "Epoch 58, Loss: 218.99476623535156\n",
      "Epoch 59, Loss: 232.599609375\n",
      "Epoch 60, Loss: 308.2842102050781\n",
      "Epoch 61, Loss: 208.47476196289062\n",
      "Epoch 62, Loss: 188.4884033203125\n",
      "Epoch 63, Loss: 284.94927978515625\n",
      "Epoch 64, Loss: 162.36917114257812\n",
      "Epoch 65, Loss: 185.8701934814453\n",
      "Epoch 66, Loss: 240.8550567626953\n",
      "Epoch 67, Loss: 234.37945556640625\n",
      "Epoch 68, Loss: 290.4315490722656\n",
      "Epoch 69, Loss: 167.29168701171875\n",
      "Epoch 70, Loss: 328.5489501953125\n",
      "Epoch 71, Loss: 311.02459716796875\n",
      "Epoch 72, Loss: 287.764404296875\n",
      "Epoch 73, Loss: 223.87025451660156\n",
      "Epoch 74, Loss: 268.5302429199219\n",
      "Epoch 75, Loss: 262.22259521484375\n",
      "Epoch 76, Loss: 183.6259002685547\n",
      "Epoch 77, Loss: 230.39068603515625\n",
      "Epoch 78, Loss: 249.0763702392578\n",
      "Epoch 79, Loss: 253.78585815429688\n",
      "Epoch 80, Loss: 221.7920379638672\n",
      "Epoch 81, Loss: 199.0615692138672\n",
      "Epoch 82, Loss: 203.81475830078125\n",
      "Epoch 83, Loss: 253.66944885253906\n",
      "Epoch 84, Loss: 179.20355224609375\n",
      "Epoch 85, Loss: 222.5283203125\n",
      "Epoch 86, Loss: 217.65603637695312\n",
      "Epoch 87, Loss: 223.03179931640625\n",
      "Epoch 88, Loss: 199.4697723388672\n",
      "Epoch 89, Loss: 302.2703857421875\n",
      "Epoch 90, Loss: 280.27227783203125\n",
      "Epoch 91, Loss: 187.884765625\n",
      "Epoch 92, Loss: 250.74998474121094\n",
      "Epoch 93, Loss: 302.27734375\n",
      "Epoch 94, Loss: 219.09384155273438\n",
      "Epoch 95, Loss: 264.46307373046875\n",
      "Epoch 96, Loss: 214.2308349609375\n",
      "Epoch 97, Loss: 257.50079345703125\n",
      "Epoch 98, Loss: 230.37770080566406\n",
      "Epoch 99, Loss: 235.52587890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147104/2009598924.py:16: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(y_pred, y_batch)\n"
     ]
    }
   ],
   "source": [
    "# write a training loop using the SGD algorithm to find the optimal value wâˆ—. \n",
    "# Use a constant learning rate for all iterations\n",
    "\n",
    "W = torch.randn(1, 2) * 10\n",
    "W.requires_grad_()\n",
    "B = 40 #batch size\n",
    "for iter in range(max_iter):\n",
    "    perm = torch.randperm(X.shape[0])\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    for i in range(0, X.shape[0], B):\n",
    "        X_batch = X[i:i+B, :]\n",
    "        y_batch = y[i:i+B]\n",
    "\n",
    "        y_pred = X_batch @ W.T\n",
    "        loss = torch.nn.functional.mse_loss(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            W -= init_lr * W.grad\n",
    "        # zero the gradients after updating\n",
    "        W.grad.zero_()\n",
    "\n",
    "    print(f'Epoch {iter}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e0b7c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c522f8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2324, -0.0690]], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
