{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1270b323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda installed! Running on GPU!\n",
      "Device: cuda:0 Quadro K2200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set all random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "\n",
    "def set_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "        print(\"Cuda installed! Running on GPU!\")\n",
    "        device = torch.device(torch.cuda.current_device())\n",
    "        print(f'Device: {device} {torch.cuda.get_device_name(device)}')\n",
    "    else:\n",
    "        print(\"No GPU available!\")\n",
    "    return device\n",
    "\n",
    "set_seed(100)\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3819f936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read MNIST training data\n",
    "df = pd.read_csv('data/mnist_train.csv')\n",
    "X = df.iloc[:, 1:].to_numpy() / 255.0       # values are scaled to be between 0 and 1\n",
    "y = df.iloc[:, 0].to_numpy()                # labels of images\n",
    "\n",
    "# write a binary classifier that classifies whether the digit is 5 or not\n",
    "y_binary = (y == 5).astype(int)\n",
    "\n",
    "# plot the first dozen images from the data set\n",
    "plt.figure()\n",
    "for i in range(12):\n",
    "    plt.subplot(3, 4, i+1, xticks=[], yticks=[])\n",
    "    image = X[i, :].reshape((28,28))\n",
    "    plt.imshow(image, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d0628cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in dJ/dw: tensor(5.3828, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Difference in dJ/db: tensor([0.8995], grad_fn=<AbsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# convert to torch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_binary, dtype=torch.float32)\n",
    "\n",
    "# make w and b require gradients and be randomly initialized\n",
    "w_tensor = torch.rand(784, dtype=torch.float32, requires_grad=True)\n",
    "b_tensor = torch.rand(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def model(X, w, b):\n",
    "    return torch.sigmoid(torch.matmul(X, w) + b)\n",
    "\n",
    "def loss_fn(y_pred, y):\n",
    "    eps = 1e-8\n",
    "    return -torch.mean(y * torch.log(y_pred + eps) + (1 - y) * torch.log(1 - y_pred + eps))\n",
    "\n",
    "# compute the initial loss function gradient at (w, b) using Pytorch backpropagation\n",
    "y_pred_tensor = model(X_tensor, w_tensor, b_tensor)\n",
    "loss = loss_fn(y_pred_tensor, y_tensor)\n",
    "loss.backward()\n",
    "\n",
    "# compute the initial loss function gradient at (w, b) mannually\n",
    "dJ_dw = torch.matmul(model(X_tensor, w_tensor, b_tensor) - y_tensor, X_tensor) / X_tensor.shape[0]\n",
    "dJ_db = torch.sum(model(X_tensor, w_tensor, b_tensor) - y_tensor) / X_tensor.shape[0]\n",
    "\n",
    "# compare the two gradients\n",
    "print(\"Difference in dJ/dw:\", torch.norm(dJ_dw - w_tensor.grad))\n",
    "print(\"Difference in dJ/db:\", torch.abs(dJ_db - b_tensor.grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "06f4c1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic gradient dJ/dw: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0094, 0.0231, 0.0013, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0029, 0.0295, 0.0298, 0.0120, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0004, 0.0186, 0.0299, 0.0298, 0.0123, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0032, 0.0219, 0.0206, 0.0298, 0.0066, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0005, 0.0175, 0.0045, 0.0248, 0.0298, 0.0020, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0071, 0.0187, 0.0000, 0.0299, 0.0298, 0.0020,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0108, 0.0018, 0.0000, 0.0299, 0.0298,\n",
      "        0.0020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0061, 0.0000, 0.0008, 0.0299,\n",
      "        0.0234, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0104,\n",
      "        0.0299, 0.0175, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0174, 0.0299, 0.0113, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0007, 0.0267, 0.0300, 0.0009, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0011, 0.0298, 0.0241, 0.0004, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0072, 0.0298, 0.0206, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0079, 0.0298, 0.0185, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0096, 0.0298, 0.0104, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0047, 0.0298, 0.0104, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0011, 0.0298, 0.0114,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0011, 0.0291,\n",
      "        0.0245, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0128, 0.0299, 0.0053, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0227, 0.0272, 0.0049, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000])\n",
      "Stochastic gradient dJ/db: tensor([0.0300])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149823/1626184039.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X[indices, :], dtype=torch.float32)\n",
      "/tmp/ipykernel_149823/1626184039.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y[indices], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent\n",
    "# using a batch size of b = 32, write a function that returns a stochastic gradient of J by choosing b\n",
    "# randomly chosen images from the dataset.\n",
    "\n",
    "def stochastic_gradient(X, y, w, b, batch_size=32):\n",
    "    indices = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
    "    X_batch = torch.tensor(X[indices, :], dtype=torch.float32)\n",
    "    y_batch = torch.tensor(y[indices], dtype=torch.float32)\n",
    "    w.grad = None\n",
    "    b.grad = None\n",
    "    y_pred_batch = model(X_batch, w, b)\n",
    "    loss = loss_fn(y_pred_batch, y_batch)\n",
    "    loss.backward()\n",
    "    return w.grad, b.grad \n",
    "\n",
    "# test the stochastic gradient function\n",
    "dJ_dw_sgd, dJ_db_sgd = stochastic_gradient(X_tensor, y_tensor, w_tensor, b_tensor, batch_size=32)\n",
    "print(\"Stochastic gradient dJ/dw:\", dJ_dw_sgd)\n",
    "print(\"Stochastic gradient dJ/db:\", dJ_db_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "86b8511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in dJ/dw: tensor(0.0054)\n",
      "Difference in dJ/db: tensor([0.0002])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149823/1626184039.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X[indices, :], dtype=torch.float32)\n",
      "/tmp/ipykernel_149823/1626184039.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y[indices], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# compute the full gradient using the entire dataset\n",
    "def full_gradient(X, y, w, b):\n",
    "    w.grad = None\n",
    "    b.grad = None\n",
    "    y_pred = model(X, w, b)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    return w.grad, b.grad\n",
    "\n",
    "# test the full gradient function\n",
    "dJ_dw_full, dJ_db_full = full_gradient(X_tensor, y_tensor, w_tensor, b_tensor)\n",
    "\n",
    "# call the stochastic gradient function multiple times and average the results\n",
    "num_samples = 100\n",
    "dJ_dw_avg = torch.zeros_like(w_tensor)\n",
    "dJ_db_avg = torch.zeros_like(b_tensor)\n",
    "for _ in range(num_samples):\n",
    "    dJ_dw_sgd, dJ_db_sgd = stochastic_gradient(X_tensor, y_tensor, w_tensor, b_tensor, batch_size=32)\n",
    "    dJ_dw_avg += dJ_dw_sgd / num_samples\n",
    "    dJ_db_avg += dJ_db_sgd / num_samples\n",
    "\n",
    "# compare the average stochastic gradient with the full gradient\n",
    "print(\"Difference in dJ/dw:\", torch.norm(dJ_dw_avg - dJ_dw_full))\n",
    "print(\"Difference in dJ/db:\", torch.abs(dJ_db_avg - dJ_db_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a64ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
